{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d20e715",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Critical Scale Invariance in a Healthy Human Heart Rate\n",
    "<hr>\n",
    "\n",
    "> Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari\n",
    "\n",
    "## Abstract\n",
    "In this notebook we will reproduce the analysis made by Kiyono in this Physical Review Letter [Kiyono, 1](https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.93.178103). We will investigate the probability distribution function of heart beat time intervals recorded from healty individuals. A model inspired by high Reynolds number turbolence effects on fluid velocities, taken from Castaing's work, will be used to fit the data and test the scale invariance of the distribution of interbeats times. Rhythms from unhealty individuals will be also taken into account to test whether they follow the same distibutions and trends, thus providing hints to pathological or life threathening diagnosis conditions.\n",
    "\n",
    "# Healthy Heart Rate Variability and its Probability Distribution\n",
    "Human heart rate is a complex biological signal, whose statistical properties are deeply studied not only from a medical point of view. i.e. to assess the patient's medical conditions, but also from a mathematical perspective.\n",
    "\n",
    "Here we will mainly focus our attention to the probability distribution of the cardiac interbeat times, defined as the time differences between consecutives heart contractions. Such PDF does not follow a Gaussian statistic [Peng, 2](https://link.aps.org/doi/10.1103/PhysRevLett.70.1343) and more importantly displays a robust scale invariance, hence suggesting the idea that a healthy cardiac system operates near a critical and out of equilibrium state [Yvanov, 3](https://www.nature.com/articles/20924).\n",
    "\n",
    "To further investigate these hypotheses, unhealthy rythms will also be analyzed. Some studies suggest the presence of deviations from the healthy PDF could be used as a possible tool for cardiac health check [Bigger, 4](https://www.ahajournals.org/doi/10.1161/01.CIR.93.12.2142), as discrepancies from a healthy critical behaviour may reduce the overall efficiency of transporation phenomena in a similar fashion to what happens in other physical systems [Takayasu, 5](https://www.sciencedirect.com/science/article/pii/S0378437199004999).\n",
    "\n",
    "\n",
    "# Experimental measurements of Heart Beats\n",
    "The data used to reproduce the analysis are taken from PhysioNet.org public database. Three different datasets are here analyzed:\n",
    "\n",
    "- [\"Fantasia\" dataset](https://doi.org/10.13026/C2RG61), made of  recordings from 20 young (21-34 years old) and 20 elder (68 - 85 years old) individuals. All subject were selected after a strict health check to check the absence of any pathological rythms. The ECG signals were taken for a total of 120 minutes, sampled at 250 Hz, while each participant was watching the Disney movie Fantasia in order to help mantain wakefulness. Together with ECG signals, respiration and only in some individuals blood pressure were also recorded.\n",
    "- [Normal Sinus Rhythm dataset](https://doi.org/10.13026/C2NK5R), which includes 18 long-term (~22 hours) ECG recordings of subjects found to have had no significant arrhythmias; they include 5 men, aged 26 to 45, and 13 women, aged 20 to 50. Electric signals were sampled at 128 Hz.\n",
    "- [Congestive Heart Failure dataset](https://doi.org/10.13026/C29G60) which includes long-term ECG recordings from 15 subjects (11 men, 22-71 y.o., and 4 women, 54-63 y.o.) with severe congestive heart failure. Each recording lasts 20 hours and data is sampled at 250 Hz with an ambulatory ECG recorder.\n",
    "\n",
    "The measurements record the electrical activity of the heart by the usage of ECG electrodes sticked to patient's skin. The higher sampling rate (128 or 250 Hz) with respect to the contraction cycles allows a finer analisys up to phenomena of time order bigger than 0.008 seconds.\n",
    "\n",
    "The electrical activity recorded originates from the response of the cardiac muscle to the stimulus first sent by peace maker cells in the Sinoatrial node. The signal therefore is the overall response of different parts of the heart activating at different times, partly modulated by the anatomy of the tissues.  \n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/heart_pulse.png\" alt=\"heart_pulse_signal\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"images/ECG.gif\" alt=\"GIF ECG\" style=\"width: 250px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "It is possible to see the units of measure of the signal, $mV$ and the time order of magnitude over which the signal lasts. More importantly the different phases of the signal are here recognizable, also by comparing the animation above. \n",
    "The first small signal corresponds to the P phase and is related to the electrical activity originating from the Sinoatrial node. Then the so called QRS complex follows and during this time the maximum electrical activity corresponding to the R peak occurs which will be exploited by the detection algorithms to compute the exact beat time. Finally the depolarization (contractions) of ventricules happens in the following SP phase ad the cycle then repeats. In reality the peak detection algorithms record the R peak times and not the contractions times, but since the delay between these two instants is approximately the same and its variations are negligible with respect to the interbeat time, the extrapolated durations are still statistically significant and a robust analisys can be carried out.\n",
    "\n",
    "\n",
    "# Datasets and Tools\n",
    "Here the datasets used will be briefely shown to explain their future usage in the analysis. To access data files `WFDB` Python package was used. It is an open source library specifically built to interface with PhysioNet datasets and provides powerful methods of peak estimation on raw ECG files, allowing easy visualization shortucuts and an high level of customization and configuration. \n",
    "\n",
    "<table><tr>\n",
    "    <td><figure><img src=\"images/fantasia_patient_distribution.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "  <figcaption>Fig.1 - Fantasia</figcaption>\n",
    "</figure></td>\n",
    "        <td><figure><img src=\"images/mit_patient_distribution.png\" alt=\"congestive\" style=\"width:100%\">\n",
    "  <figcaption>Fig.2 - Normal Sinus</figcaption>\n",
    "</figure></td>\n",
    "        <td><figure><img src=\"images/congested_patient_distribution.png\" alt=\"congestive\" style=\"width:100%\">\n",
    "  <figcaption>Fig.3 - Congestive Heart Failure</figcaption>\n",
    "</figure></td>\n",
    "</tr></table>\n",
    "\n",
    "## Fantasia\n",
    "Fantasia dataset contains recordings from 40 different subjects. The first 20 subject's age is ranges from 21 to 34 years old and the corresponding files are marked with the letter \"y\". The remaining subjects'age ranges from 68 to 86 years old and the letter \"o\" is used instead, as shown in Fig.1.\n",
    "\n",
    "Data was stored according to the format provided by `.hea` files. The raw ECG data was stored in `.dat` format and it was loaded with `WFDB` and later converted into a more easily manageble Pandas DataFrame. Blood pressure and respiratory data are here neglected and not taken into account on the following analysis.\n",
    "ECG data are stored as an ordered sequence of values of voltage differences, sampled at 250 Hz with a conversion factor of 2000 adu/mV (analog digital units). For each patient there is a precomputed annotation file, containing the sample indexes for witch the ECG signal peaks, i.e. at the R peaks.\n",
    "\n",
    "<table><tr>\n",
    "    <td><figure><img src=\"images/ECG1.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "  <figcaption>Fig.4 - Single Ecg pulse </figcaption>\n",
    "</figure></td>\n",
    "        <td><figure><img src=\"images/ECG6.png\" alt=\"congestive\" style=\"width:100%\">\n",
    "  <figcaption>Fig.5 - A series of consecutive pulses </figcaption>\n",
    "</figure></td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "## Normal Sinus\n",
    "This database includes 18 24-hours long ECG recordings of subjects, sampled at 128 Hz. Subjects included in this database were found to have had no significant arrhythmias, and age and sex distribution is shown in Fig.2. \n",
    "This dataset will be useful to check the result of the previous Fantasia recordings, giving more accurate information as there are much more data to use. The signals were still recorded with two leads, but to speed up the analysis process, annotations already available will be used.\n",
    "\n",
    "## Congestive Heart Failure\n",
    "This datset collects data from long-term ECG recordings of 15 subjects, whose age and sex distribution is shown in Fig.3 above. The individual recordings are each 20 hours in duration, and contain two _leads_ ECG signals each sampled at 250 Hz with 12-bit resolution over a range of ±10 millivolts. The patients suffered from severe congestive heart failure (NYHA class 3–4), a chronic progressive condition that affects the pumping power of the heart muscle, thus causing a build up of fluids and a significant reduction of efficiency of the organ.\n",
    "\n",
    "Data were recorded using two different leads, thus the signal available is a bit different than Fantasia. It is still possible however to compute the interbeat times as the periodic signal displays clear absolute maxima in each pulse. Unchecked annotations of beat indexes were available and these were used in place of running the peak detection algorithm.\n",
    "\n",
    "We decided to check congestive heart failure dataset to verifiy if there are any differences in the interbeat times distribution, as argued in [Bigger, 4](https://www.ahajournals.org/doi/10.1161/01.CIR.93.12.2142)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549db198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for interfacing with WFDB data\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "\n",
    "\n",
    "\n",
    "#Classical libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import curve_fit\n",
    "from numpy import trapz\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "# importing detrending library\n",
    "import obspy\n",
    "from obspy.signal import detrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bae3d",
   "metadata": {},
   "source": [
    "# The databases are: Fantasia, MIT, Chfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807f2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "database='Fantasia' \n",
    "path = paths[database]\n",
    "key = keys[database]\n",
    "files= np.loadtxt(path+'RECORDS', dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d802b",
   "metadata": {},
   "source": [
    "# Setting possible Path and Key for each Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ee0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {'Fantasia':'physionet.org/files/fantasia/1.0.0/', 'MIT':'physionet.org/files/nsrdb/1.0.0/', 'Chfdb': 'physionet.org/files/chfdb/1.0.0/'}\n",
    "keys= {'Fantasia':'RR_intervals_original_Fantasia', 'MIT':'RR_intervals_original_MIT', 'Chfdb': 'RR_intervals_original_Chfdb'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f3d89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculations of RR Intervals\n",
    "This function its used to store the ECG data inside the original files into a Pandas Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_ecg(path, files):\n",
    "    \n",
    "    '''\n",
    "    Return ECG and RR_intervals dataframe for Fantasia.\n",
    "    Return only RR_intervals dataframe for MIT and Chfdb\n",
    "\n",
    "    Inputs:\n",
    "        path: path to the data's files\n",
    "        files: files name without extentions\n",
    "    \n",
    "    outputs:\n",
    "    return:\n",
    "        ECG_sig: DataFrame with EGC signal for each file in each columns\n",
    "        RR_intervals: DataFrame where in each columns are stored the interbeats intervals for each files\n",
    "    '''\n",
    "\n",
    "\n",
    "    # DataFrames to handle all signals (RAW ECG, RR intervals, Annotations, ...)\n",
    "    ECG_sig=pd.DataFrame(columns=files)\n",
    "\n",
    "    RR_intervals = pd.DataFrame(columns=files)\n",
    "\n",
    "    # Read ECG data from files and store them in Pandas DataFrame\n",
    "    # Save fields too, for further optional analysis\n",
    "    signals_dfs=[]\n",
    "    rr_dfs=[]\n",
    "\n",
    "    if path == paths['Fantasia']: \n",
    "        for file in files:\n",
    "                sig, fields =  wfdb.rdsamp(path + file, channels=[1]) # channel 1 maps to ECG data\n",
    "                signals_dfs.append(pd.DataFrame({file: sig.T[0]}))\n",
    "\n",
    "                xqrs = processing.XQRS(sig=-sig[:,0], fs=fields['fs'])\n",
    "                xqrs.detect()\n",
    "                rr=processing.calc_rr(xqrs.qrs_inds, fs=fields['fs'])\n",
    "            \n",
    "                rr_dfs.append(pd.DataFrame({file: rr}))\n",
    "\n",
    "        ECG_sig=pd.concat(signals_dfs, ignore_index=True, axis=1)\n",
    "        ECG_sig.columns=files\n",
    "\n",
    "        RR_intervals=pd.concat(rr_dfs, ignore_index=True, axis=1)\n",
    "        RR_intervals.columns=files\n",
    "        \n",
    "        return ECG_sig, RR_intervals\n",
    "    \n",
    "    else: \n",
    "        for file in files: \n",
    "            rr = processing.ann2rr(path+file, extension='ecg')\n",
    "            rr_dfs.append(pd.DataFrame({file: rr}))\n",
    "        \n",
    "        RR_intervals=pd.concat(rr_dfs, ignore_index=True, axis=1)\n",
    "        RR_intervals.columns=files\n",
    "    \n",
    "    return RR_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb34cb82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cleaning function\n",
    " This function find problematic files and uses the annotation files to correct them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39881f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(RR_intervals, path_to_files):\n",
    "    \n",
    "    RR = RR_intervals.copy()\n",
    "    files = list(RR.columns)\n",
    "    \n",
    "    #Extracting the problematc files name\n",
    "    problematic_files = list(RR.columns[RR[files].sum() == 0])\n",
    "    \n",
    "    #Removing from the file list the problematic ones\n",
    "    files = list(RR.columns[RR[files].sum() != 0])\n",
    "    \n",
    "    #Removing the columns which bare no value\n",
    "    RR = RR.drop(problematic_files, axis=1)\n",
    "    \n",
    "    #Adding the new columns to the dataframe and the files name to the list\n",
    "    for file in problematic_files:\n",
    "        rr = processing.ann2rr(path_to_files+file, extension='ecg')\n",
    "        RR = pd.concat([RR, pd.Series(rr, name=file)], axis=1)\n",
    "\n",
    "    new_files = files + problematic_files\n",
    "    \n",
    "    return RR, new_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e3bb3",
   "metadata": {},
   "source": [
    "# This cell is meant to run only when first importing the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5a9af79",
   "metadata": {},
   "source": [
    "if database=='Fantasia': \n",
    "    ECG_sig, RR_intervals_original = import_ecg(path, files)\n",
    "\n",
    "    ECG_sig.to_hdf('ECG_sig.hdf', key=key)\n",
    "    RR_intervals_original.to_hdf('RR_intervals_original.hdf', key=key)\n",
    "\n",
    "else:\n",
    "    RR_intervals_original = import_ecg(path, files)\n",
    "    RR_intervals_original.to_hdf('RR_intervals_original.hdf', key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fea83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plots of signals\n",
    "For all dataset \n",
    "<table><tr>\n",
    "    <td><figure><img src=\"images/b_Fantasia.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "  <figcaption>Fig.6 - Interbeat times for Fantasia dataset </figcaption>\n",
    "</figure></td>\n",
    "    <td><figure><img src=\"images/b_MIT.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "  <figcaption>Fig.7 - Interbeat times for MIT dataset </figcaption>\n",
    "</figure></td>\n",
    "    <td><figure><img src=\"images/b_Chfdb.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "  <figcaption>Fig.8 - Interbeat times for Chfdb dataset </figcaption>\n",
    "</figure></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d95d004",
   "metadata": {},
   "source": [
    "RR = pd.read_hdf('RR_intervals_original.hdf', key=key)\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(RR['chf08']/250, color=colors[8], label='b(i)')\n",
    "plt.grid(linestyle='dotted')\n",
    "plt.xlabel('Beat number')\n",
    "plt.ylabel('Intebeat time [s]')\n",
    "plt.legend('right down')\n",
    "#plt.savefig('b_Chfdb.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd8ded",
   "metadata": {},
   "source": [
    "## Loading the data inside a Dataframe for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e542cb4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_792/3886252514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRR_intervals_original\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RR_intervals_original.hdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "RR_intervals_original=pd.read_hdf('RR_intervals_original.hdf', key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757f65b",
   "metadata": {},
   "source": [
    "# Clensing the dataframe from the Nan columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e61b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_intervals, files = clean_df(RR_intervals_original, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661d419",
   "metadata": {},
   "source": [
    "## We clean the data redefining the ones that exede the interval (mean - f x std, mean+f x std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f95cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "factor=3\n",
    "\n",
    "for file in files:\n",
    "    mean = RR_intervals[file].mean()\n",
    "    std  = RR_intervals[file].std()\n",
    "\n",
    "    RR_intervals[file][(RR_intervals[file] > mean + factor*std) | ( RR_intervals[file]< mean - factor*std)]=mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5dfea0",
   "metadata": {},
   "source": [
    "# Constructing the cumulative functions from the beats intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B=pd.DataFrame(np.cumsum(RR_intervals, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26488a",
   "metadata": {},
   "source": [
    "# Constructing the cumulative functions from the beats intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3acdd",
   "metadata": {},
   "source": [
    "## Defining the values for s and computing the increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S=[2**i for i in range(3, 11)]\n",
    "list_Delta_B= increments(B, S, polydegree=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d218d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation of  $\\Delta_s B(i)$\n",
    "Function to compute the detrended data and then the increments from the comulative function. In order to do that the B(m) comulative is pass to the function as input. The detranding on different scales s is done with the `detrend.polynomial` function which is imported from the obspy library (see Appendix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def increments(B_df, s_list, polydegree=3):\n",
    "    '''\n",
    "    Inputs:\n",
    "        B: Dataframe with the columns to be detrended\n",
    "        s_list: List of value of s to be used to detrend B\n",
    "        polydegree: Polynomial's degree used to detred the data\n",
    "    \n",
    "    Output:\n",
    "        return : List of array containing the detrended data increments for each column of B\n",
    "    \n",
    "    '''\n",
    "    fnames = list(B_df.columns) #columns name list\n",
    "    list_DB=[]\n",
    "    \n",
    "    for s in s_list:\n",
    "        \n",
    "        Delta_B_s=[] #empy list to store the increamets for a specific s regardless of the file from which the data come from\n",
    "\n",
    "        \n",
    "        for B in B_df[fnames].values.T:\n",
    "            \n",
    "            B = B[~np.isnan(B)] #removing all nan values from B     \n",
    "            nmax_seg=int(np.floor(B.size/(2*s))) #maximum number of possible segments with increment s over B\n",
    "            \n",
    "            \n",
    "            if nmax_seg!= 0: #avoid useless computing power\n",
    "                \n",
    "                for indx in range(nmax_seg):\n",
    "                    detrend.polynomial(B[indx*2*s: (indx+1)*2*s], order=polydegree, plot=False) # for each sliding segment, detrend\n",
    "\n",
    "                # the whole B but now detrended\n",
    "                #detrended=B[0:2*s*nmax_seg].copy() #only those that are possible to detrend (have at least the 2s space/segment)\n",
    "                detrended=B[0:2*s*nmax_seg]\n",
    "                Delta_s_B_file=np.array([detrended[i+s]-detrended[i] for i in range(2*s*nmax_seg) if i+s< len(detrended)])\n",
    "                Delta_B_s.append(Delta_s_B_file/np.std(Delta_s_B_file)) #since the std of very patient i svery different from each other we need to normalize the increments and join them togheter\n",
    "        list_DB.append(np.concatenate(Delta_B_s))\n",
    "    \n",
    "    return list_DB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810f99d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd8a6f",
   "metadata": {},
   "source": [
    "Following the steps of the paper we fit the Gaussian's and Castaing's model to our datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e5591",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian models\n",
    "The gaussian model fit well the peak of the distribution of the $\\Delta_s B(i)$, while it fails to fit the tails. This is expected from the theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd2abb",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "    <td><figure><img src=\"images/Gaussian_fit_Fantasia.png\" alt=\"fantasia\" style=\"width:100%\">\n",
    "<figcaption>Fig.9 - Gaussian fit for Fantasia dataset </figcaption>\n",
    "</figure></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abd4b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Castaing's model\n",
    "Non gaussian model, explanation of the formula, where it comes from, the physical meaning of the parameters, how the fit was implemented on the data, how good the fit is, residuals graph\n",
    "Explain normalisations choices used, in log scale a division is just a vertical offset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359fa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df00e9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unhealthy individuals\n",
    "Does the fit looks like the previous, how does it change, does it change significantly\n",
    "\n",
    "# Scale invariance\n",
    "## Collapse plot on the histogram distributions\n",
    "Show the collapse plot of the data, use other papers cited in literature, fluctuations at different scales\n",
    "\n",
    "## Dependence on s parameter\n",
    "What do we expect, plot how it changes when s is varied, change detrending order and see if there are any variations\n",
    "\n",
    "# Multifractality nature of the Interbeat Heart Times\n",
    "\n",
    "\n",
    "# Bibliography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3210da59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
